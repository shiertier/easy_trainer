# LLM 模型选择
JOY2_MODEL_STR = "/mnt/nvme/chenyu-nvme/ComfyUI/models/LLM/llama-joycaption-alpha-two-hf-llava" # -nf4

# 量化设置
# JOY2_IS_NF4_BOOL = True # 改为通过模型名获取

# 使用gpu
JOY2_GPU_COUNT_STR = "all"

# 将单个模型部署多份还是部署一份
# JOY2_MODEL_IN_PER_GPU_BOOL = False

JOY2_MAX_BATCH_SIZE = 1

